import streamlit as st
import os
import tempfile
from langchain_groq import ChatGroq
# Mudan√ßa: Vamos usar o pypdf direto em vez do loader do langchain
from pypdf import PdfReader
from langchain.docstore.document import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(page_title="Chat com PDF (RAG)", page_icon="üß†", layout="wide")

# --- CSS Personalizado ---
st.markdown("""
<style>
    .stChatMessage { border-radius: 10px; padding: 10px; }
    .stButton>button { width: 100%; background-color: #7c3aed; color: white; }
</style>
""", unsafe_allow_html=True)

# --- CONFIGURA√á√ÉO DA CHAVE ---
# A chave que voc√™ forneceu anteriormente
api_key = "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"

# --- Barra Lateral ---
with st.sidebar:
    st.header("üß† Configura√ß√£o")
    st.success("‚úÖ Chave de API Embutida")
    
    st.markdown("---")
    st.info("Este sistema l√™ seu PDF, cria um √≠ndice de busca e usa IA para responder perguntas baseadas no documento.")
    if st.button("Limpar Hist√≥rico"):
        st.session_state.messages = []
        st.rerun()

# --- Fun√ß√µes de RAG (C√©rebro do App) ---

@st.cache_resource
def get_embeddings():
    # Usa modelo gratuito e leve rodando na CPU
    return HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

def process_pdf(uploaded_file):
    # Cria arquivo tempor√°rio
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    try:
        # 1. Carregar (M√âTODO ROBUSTO - MANUAL)
        # Substitu√≠mos o PyPDFLoader por uma leitura direta para evitar erro de 'bbox'
        reader = PdfReader(tmp_path)
        documents = []
        
        for i, page in enumerate(reader.pages):
            try:
                text = page.extract_text()
                if text:
                    # Criamos o objeto Document manualmente
                    doc = Document(page_content=text, metadata={"page": i + 1})
                    documents.append(doc)
            except Exception as e:
                # Se uma p√°gina der erro, pulamos ela e avisamos, mas n√£o travamos o app
                print(f"Erro ao ler p√°gina {i+1}: {e}")
                continue

        if not documents:
            raise ValueError("N√£o foi poss√≠vel extrair texto deste PDF. Ele pode ser uma imagem digitalizada.")

        # 2. Dividir (Chunking)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)

        # 3. Criar Banco Vetorial
        embeddings = get_embeddings()
        db = FAISS.from_documents(texts, embeddings)
        
        return db
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# --- Interface Principal ---
st.title("üß† Converse com seus Documentos")
st.markdown("Fa√ßa upload de um **PDF** e tire d√∫vidas com a Intelig√™ncia Artificial.")

# Inicializa hist√≥rico
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# √Årea de Upload
uploaded_file = st.file_uploader("Carregar Documento", type="pdf")

if uploaded_file:
    # Processa o arquivo apenas se o bot√£o for clicado
    if st.button("üöÄ Processar Documento"):
        with st.spinner("Lendo e indexando..."):
            try:
                st.session_state.vector_db = process_pdf(uploaded_file)
                st.success("Documento pronto! Pergunte abaixo.")
            except Exception as e:
                st.error(f"Erro ao processar: {e}")
                st.warning("Dica: Se o PDF for uma imagem digitalizada (scanner), este m√©todo n√£o funcionar√°. Use PDFs nativos.")

# √Årea de Chat
if st.session_state.vector_db:
    # Exibe hist√≥rico
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Input do usu√°rio
    if prompt := st.chat_input("Pergunte sobre o arquivo..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                try:
                    llm = ChatGroq(groq_api_key=api_key, model_name="llama3-70b-8192")
                    
                    qa_chain = RetrievalQA.from_chain_type(
                        llm=llm,
                        chain_type="stuff",
                        retriever=st.session_state.vector_db.as_retriever(search_kwargs={"k": 3}),
                        return_source_documents=True
                    )
                    
                    response = qa_chain.invoke({"query": prompt})
                    answer = response['result']
                    
                    st.markdown(answer)
                    
                    with st.expander("üìö Fontes Consultadas"):
                        for doc in response['source_documents']:
                            st.caption(f"Conte√∫do: {doc.page_content[:150]}...")
                            st.caption(f"P√°gina: {doc.metadata.get('page', 'N/A')}")
                            st.divider()
                            
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    st.error(f"Erro ao gerar resposta: {e}")

elif not uploaded_file:
    st.info("üëÜ Comece enviando um arquivo PDF.")
